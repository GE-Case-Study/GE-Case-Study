---
title: "GE Project"
output: html_document
date: "2022-11-09"
author: "Samantha Erne, Jake Holroyd, Jersie Thomas"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Creating the Tables in R

We started the project by uploading the data from the 7 CSV files that were uploaded in MySQL. 

```{r}
pacman::p_load(RMySQL)

mysqlconnection = dbConnect(
  RMySQL::MySQL(),
  dbname='gedata',
  host='mysql.fsb.miamioh.edu',
  port=3306,
  user='fsbstud',
  password='fsb4you')


dbListTables(mysqlconnection)
```

Next, we created data frames in R from the data tables hosted in MySQL. We uploaded the data from each CSV file into it's own data frame. 

```{r}
engine_data_aic= RMySQL::dbReadTable(mysqlconnection, name= "engine_data_aic")
engine_data_axm= RMySQL::dbReadTable(mysqlconnection, name= "engine_data_axm")
engine_data_fron= RMySQL::dbReadTable(mysqlconnection, name= "engine_data_fron")
engine_data_pgt= RMySQL::dbReadTable(mysqlconnection, name= "engine_data_pgt")
esn_rul= RMySQL::dbReadTable(mysqlconnection, name= "esn_rul")
lkp_airport_codes_t= RMySQL::dbReadTable(mysqlconnection, name= "lkp_airport_codes_t")
manufacturing_sql_by_esn= RMySQL::dbReadTable(mysqlconnection, name= "manufacturing_sql_by_esn")
```

Then, we set each dataset as its own data frame in R. 

```{r}
engine_data= rbind(engine_data_aic, engine_data_axm, engine_data_fron, engine_data_pgt)
```

# Joining the esn_rul table
Now, we joined the esn_rul table to the table with all of the engine data using an inner join. 

```{r}
library(dplyr)
esn_joined_data= inner_join(engine_data, esn_rul, na_matches= 'na') # gives only training data
```

# Joining the manufacturing_sql_by_esn table
We then joined the manufacturing data table to the combined table using an inner join. 

```{r}
join_data= inner_join(esn_joined_data, manufacturing_sql_by_esn, na_matches= 'na') 
```

# Imputing Nulls

The reasoning for this step is discussed under the "Consistent Data" header, it just needs to be placed here for the dataset to be created properly. 


```{r}
for (i in 1:length(join_data$depart_icao)) {
  if(join_data$depart_icao[i] == "")
    join_data$depart_icao[i] <- join_data$destination_icao[i-1]
  if(join_data$destination_icao[i] == "")
    join_data$destination_icao[i] <- join_data$depart_icao[i]}
```


# Joining the lkp_airport_codes_t table
Finally, we joined the airport codes table to our combined table using a set of inner joins. We joined this table on both the depart_icao and the destination_icao columns since we need the coordinates for both airports in order to calculate the distance between airports.

```{r}
airport_depart= left_join(join_data, lkp_airport_codes_t, by= c("depart_icao"= "airport_icao"))
airport_dest= left_join(airport_depart, lkp_airport_codes_t, by= c("destination_icao"= "airport_icao"))
airport_final= airport_dest
```

## Calculating the Distance between Airports
Next, we used the distHaversine function from the geosphere package to calculate the distance between airports. The answer gave us the distance in meters, so we converted it to miles.

```{r}
pacman::p_load(swfscMisc)
library(swfscMisc)
pacman::p_load(geosphere)
airport_final$distance= distHaversine(p1= cbind(airport_final$longitude.x, airport_final$latitude.x), p2= cbind(airport_final$longitude.y, airport_final$latitude.y))
airport_final$distance= airport_final$distance / 1000
airport_final$distance= convert.distance(airport_final$distance, from = "km", to = "mi")
```

## Cleaning the Data

Next, we ensured that the data was tidy, technically correct, and consistent. 

# Tidy Data

We started by checking for tidiness. Through our observation of the data, we concluded that the datetime column needed to be split into date and time in order for the data to be tidy. 

```{r}
pacman::p_load(lubridate, tidyr, hms)
airport_final$datetime= as_datetime(airport_final$datetime)
Tech_Correct = separate(airport_final, datetime, c("date", "time"), sep = ' ')
```

# Technically Correct

Now that we have ensured data tidiness, we started to make sure the data was technically correct. We cleaned the column names using the clean names function and also used glimpse to check for data types. We discovered that we needed to switch the date and time columns to be of the date and time data types. We also switched the names of the latitude and longitude variables to make them easier to read and understand. 

```{r}
pacman::p_load(janitor)
library(janitor)
clean_names(Tech_Correct)
glimpse(Tech_Correct)

colnames(Tech_Correct)[41]= "depart_latitude"
colnames(Tech_Correct)[42]= "depart_longitude"
colnames(Tech_Correct)[43]= "dest_latitude"
colnames(Tech_Correct)[44]= "dest_longitude"

Tech_Correct$date = as.Date(Tech_Correct$date)
Tech_Correct$time= as_hms(Tech_Correct$time)
```

# Consistent Data

Finally, we checked our data for consistency. We found that some fields were missing information for their departure and arrival airports. Under further investigation, it seems that these planes were inspected but did not leave the airport, or possibly were delayed or took off and aborted. We ordered the data by esn and flight cycle and realized that the data fields with emoty strings for depart_icao and destination_icao and realized that the destination_icao of the record before and the depart_icao of the record after were the same. We concluded that these planes in fact did not leave the airport for some reason but were still inspected. We use for loops to impute the departure airport information of the flight directly before to get rid of the nulls and allow R to fill in the information for latitude, longitude, and distance (which is 0 for all of the planes we imputed in this case). This imputation allows us to avoid nulls and to understand that these planes did not move but their inspection values still may be relevant for predicting the remaining usueful life of the engines, therefore we opted to keep them in the dataset. 

The code we used to do this is under the "Imputing Nulls" header. We had to put it at that step in order for the nulls to correctly be imputed. 

```{r}
pacman::p_load(DataExplorer)
plot_missing(Tech_Correct)
```

## Data Validation Table

Next, we utilized the pointblank package to create a data validation table for our cleaned data set. In this table, we checked to make sure that each column was of their expected type, each column was not null, and that some column were greater than or equal to 0. The only column that did not pass the pointblank validation was time. This is because our time column is of the hms type, and not POSIXct. We have confirmed hms to be a valid data type for the time, and so this failing result can be diregarded. 

```{r}
pacman::p_load(pointblank)
act= action_levels(warn_at= 0.01, notify_at= 0.01)
agent= create_agent(tbl= Tech_Correct, actions= act)
agent %>%
  col_is_date(columns= 'date') %>% 
  col_is_posix(columns= 'time') %>% 
  col_is_integer(columns= vars(esn, unit, flight_cycle, tra, htbleed, nf_dmd, pcnfr_dmd, rul)) %>% 
  col_is_numeric(columns= vars(hpc_eff_mod, hpc_flow_mod, t2, t24, t30, t50, p2, p15, p30, nf, nc, epr, ps30, phi, nrf, nrc, bpr)) %>% 
  col_is_numeric(columns= vars(farb, w31, w32, depart_latitude, depart_longitude, dest_latitude, dest_longitude, distance)) %>%
  col_is_numeric(columns= vars(X65421P11_op630_median_first, X65421P11_op232_median_first,X54321P01_op220_median_first)) %>%
  col_is_numeric(columns= vars(X54321P01_op116_median_first, X44321P02_op420_median_first, X44321P02_op016_median_first)) %>% 
  col_is_character(columns= vars(dataset, operator, depart_icao, destination_icao)) %>%
  col_vals_gte(columns= vars(unit, flight_cycle, rul, distance), value= 0) -> 
  agent 
results= interrogate(agent)
results
results %>% export_report(filename= 'ge_project_evaluation.html')

glimpse(airport_final)
```

## Write to CSV

We finally wrote the cleaned data to a CSV file to use for the remaining steps of the project. 

```{r}
pacman::p_load(readr)
write.csv(Tech_Correct, file= "ge_cleaned_data.csv")
```


